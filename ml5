# -*- coding: utf-8 -*-
"""ml5(2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AxDI1FvWg3qgHpJ_zSxeWqkDWiv0pbxa
"""

import pandas as pd

# urldata = pd.read_csv('clean_malicious.csv')
# urldata

# data = pd.read_csv('clean_url_data.csv')
# urldata = pd.concat([data,urldata],axis=0,ignore_index=False)
# urldata.to_csv('urldata.csv', index=False)

urldata = pd.read_csv('urldata.csv')
urldata.shape

from urllib.parse import urlparse

# length of url
urldata['url_length'] = urldata['url'].apply(lambda x: len(str(x)))
urldata

# length of hostname
urldata['hostname_length'] = urldata['url'].apply(lambda x: len(urlparse(x).netloc))
urldata

# length of path
urldata['path_length'] = urldata['url'].apply(lambda x: len(urlparse(x).path))
urldata

# length of the first dictionary
urldata['fd_length'] = urldata['url'].apply(lambda x: len(urlparse(x).path.split('/')[1]) if len(urlparse(x).path.split('/'))>1 else 0)
urldata

# count of different char
# urldata['count-'] = urldata['url'].apply(lambda i: i.count('-'))
# urldata['count@'] = urldata['url'].apply(lambda i: i.count('@'))
# urldata['count?'] = urldata['url'].apply(lambda i: i.count('?'))
# urldata['count%'] = urldata['url'].apply(lambda i: i.count('%'))
# urldata['count.'] = urldata['url'].apply(lambda i: i.count('.'))
# urldata['count='] = urldata['url'].apply(lambda i: i.count('='))
urldata['count_http'] = urldata['url'].apply(lambda i: i.count('http'))
urldata['count_https'] = urldata['url'].apply(lambda i: i.count('https'))
urldata['count_www'] = urldata['url'].apply(lambda i: i.count('www'))
urldata

# count of digitals
def digit_count(url):
    count = 0
    for i in url:
        if i.isnumeric():
            count = count + 1
    return count

urldata['count_digital'] = urldata['url'].apply(lambda x: digit_count(x))
urldata

# count of letter
def digit_letter(url):
    count = 0
    for i in url:
        if i.isalpha():
            count = count + 1
    return count

urldata['count_letter'] = urldata['url'].apply(lambda x: digit_letter(x))
urldata

# Count Of Number Of Directories
urldata['count_dir'] = urldata['url'].apply(lambda x: len(urlparse(x).path.split('/')))
urldata

import re

# # Use of IP or not
# def having_ip_address(url):
#     match = re.search(
#         '(([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.'
#         '([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\/)|'  # IPv4
#         '((0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\/)' # IPv4 in hexadecimal
#         '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}', url)  # Ipv6
#     if match:
#         return 1
#     else:
#         # No matching pattern found
#         return 0
# urldata['use_of_ip'] = urldata['url'].apply(lambda x: having_ip_address(x))
# urldata

# Use of Short URL or not
def short_url(url):
    match = re.search('bit\.ly|goo\.gl|shorte\.st|go2l\.ink|x\.co|ow\.ly|t\.co|tinyurl|tr\.im|is\.gd|cli\.gs|'
                      'yfrog\.com|migre\.me|ff\.im|tiny\.cc|url4\.eu|twit\.ac|su\.pr|twurl\.nl|snipurl\.com|'
                      'short\.to|BudURL\.com|ping\.fm|post\.ly|Just\.as|bkite\.com|snipr\.com|fic\.kr|loopt\.us|'
                      'doiop\.com|short\.ie|kl\.am|wp\.me|rubyurl\.com|om\.ly|to\.ly|bit\.do|t\.co|lnkd\.in|'
                      'db\.tt|qr\.ae|adf\.ly|goo\.gl|bitly\.com|cur\.lv|tinyurl\.com|ow\.ly|bit\.ly|ity\.im|'
                      'q\.gs|is\.gd|po\.st|bc\.vc|twitthis\.com|u\.to|j\.mp|buzurl\.com|cutt\.us|u\.bb|yourls\.org|'
                      'x\.co|prettylinkpro\.com|scrnch\.me|filoops\.info|vzturl\.com|qr\.net|1url\.com|tweez\.me|v\.gd|'
                      'tr\.im|link\.zip\.net',
                      url)
    if match:
        return 1
    else:
        return 0
urldata['short_url'] = urldata['url'].apply(lambda x: short_url(x))
urldata

import matplotlib.pyplot as plt
import seaborn as sns

# Heatmap
corrmat = urldata.corr()
f, ax = plt.subplots(figsize=(25,19))
sns.heatmap(corrmat, square=True, annot = True, annot_kws={'size':10})

plt.figure(figsize=(15,5))
sns.countplot(x='label',data=urldata)
plt.title("Count Of URLs",fontsize=20)
plt.xlabel("Type Of URLs",fontsize=18)
plt.ylabel("Number Of URLs",fontsize=18)

print("Percent Of Malicious URLs:{:.2f} %".format(len(urldata[urldata['label']=='malicious'])/len(urldata['label'])*100))
print("Percent Of Benign URLs:{:.2f} %".format(len(urldata[urldata['label']=='benign'])/len(urldata['label'])*100))

plt.figure(figsize=(20,5))
plt.hist(urldata['url_length'],bins=50,color='LightBlue')
plt.xlabel("Url-Length",fontsize=18)
plt.ylabel("Number Of Urls",fontsize=18)
plt.ylim(0,1000)

plt.figure(figsize=(20,5))
plt.hist(urldata['hostname_length'],bins=50,color='LightGray')
plt.xlabel("Length Of Hostname",fontsize=18)
plt.ylabel("Number Of Urls",fontsize=18)
plt.ylim(0,1000)

# plt.figure(figsize=(15,5))
# plt.title("Use Of IP In Url",fontsize=20)
# plt.xlabel("Use Of IP",fontsize=18)

# sns.countplot(x='use_of_ip',data=urldata)
# plt.ylabel("Number of URLs",fontsize=18)

plt.figure(figsize=(15,5))
plt.title("Use Of http In Url",fontsize=20)
plt.xlabel("Use Of IP",fontsize=18)
plt.ylim((0,1000))
sns.countplot(x ='count_http',data=urldata)
plt.ylabel("Number of URLs",fontsize=18)



plt.figure(figsize=(15,5))
plt.title("Use Of http In Url",fontsize=20)
plt.xlabel("Count Of http",fontsize=18)
plt.ylabel("Number of URLs",fontsize=18)
plt.ylim((0,500))
sns.countplot(x='count_http',hue='label',data=urldata)
plt.ylabel("Number of URLs",fontsize=18)

plt.figure(figsize=(15,5))
plt.title("Use Of WWW In URL",fontsize=20)
plt.xlabel("Count Of WWW",fontsize=18)
sns.countplot(x='count_www',data=urldata)
plt.ylim(0,1000)
plt.ylabel("Number Of URLs",fontsize=18)

plt.figure(figsize=(15,5))
plt.title("Use Of WWW In URL",fontsize=20)
plt.xlabel("Count Of WWW",fontsize=18)

sns.countplot(x='count_www',hue='label',data=urldata)
plt.ylim(0,1000)
plt.ylabel("Number Of URLs",fontsize=18)

# Building Models Using Lexical Features Only

from sklearn.model_selection import train_test_split

#Independent Variables
x = urldata.iloc[: ,3:]

print(x)

#Dependent Variable
y = urldata.iloc[: , 2]
y

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

# from sklearn.svm import LinearSVC

# C_list = [0.0001,0.01,0.1,1,100]

# for c in C_list:
# # Fit the model for C
#     model= LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=1, C=c, multi_class='ovr',fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=1, random_state=None, max_iter=10000
#     ).fit(X_train, y_train)
#     print(model.score(X_train, y_train))
#     print(model.score(X_test, y_test))
#     print(model.coef_,model.intercept_)

# print(model.score(X_train, y_train))
# print(model.score(X_test, y_test))
# print(model.coef_,model.intercept_)

from sklearn.linear_model import LogisticRegression

LR_model = LogisticRegression(penalty='none',solver='lbfgs',verbose=1, max_iter=10000)
LR_model.fit(X_train,y_train)

# print(LR_model.score(X_train, y_train))
# print(LR_model.score(X_test, y_test))
# print(LR_model.coef_,model.intercept_)

from sklearn.linear_model import Lasso

Lasso_model = Lasso(alpha=0.5)
Lasso_model.fit(X_train,y_train)

# print(Lasso_model.score(X_train, y_train))
# print(Lasso_model.score(X_test, y_test))
# print(Lasso_model.coef_,model.intercept_)

from sklearn.linear_model import Ridge
alpha_list = [0.000001,0.01,0.5,1,100]
for a in alpha_list:
    Ridge_model = Ridge(alpha=a)
    Ridge_model.fit(X_train,y_train)
    print("train",Ridge_model.score(X_train, y_train))
    print(Ridge_model.score(X_test, y_test))
    print(Ridge_model.coef_,model.intercept_)

print("train",Ridge_model.score(X_train, y_train))
print(Ridge_model.score(X_test, y_test))
print(Ridge_model.coef_,model.intercept_)

print(x)

from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error,confusion_matrix,precision_score,roc_auc_score,plot_roc_curve
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
kf = KFold(n_splits=5)
mean_error_train=[]
std_error_train=[]
mean_error_test=[]
std_error_test=[]
roc_auc_scores = []
C_range = [0.000001,0.001,0.1,1,10,100,200]
for c in C_range:
#     print(c)
#     print('start')
    model = Ridge(alpha=c,normalize = True)
    temp_train=[]
    temp_test=[]
    temp_roc = []
    for train,test in kf.split(x):
        model.fit(x.iloc[train], y.iloc[train])
        ypred_test = model.predict(x.iloc[test])
        ypred_train = model.predict(x.iloc[train])
        # print(ypred_test)
        temp_test.append(mean_squared_error(y.iloc[test],ypred_test))
        temp_train.append(mean_squared_error(y.iloc[train],ypred_train))
#         print(mean_squared_error(y[test],ypred_test))
#         print("=========")
        # fig = plt.figure()
        # plot data
#         ax = Axes3D(fig)
#         ax.scatter(X[:,0] ,X[:,1] ,y)
#         ax.set_xlabel('x_1')
#         ax.set_ylabel('x_2')
#         ax.set_zlabel('target_y')
#         ax.set_title('C=',y=-0.2)
#         ax.plot_surface(Xpoly[test][:,1],Xpoly[test][:,2],ypred_test,rstride=1, cstride=1, cmap=cm.viridis, antialiased=True)
    mean_error_test.append(np.array(temp_test).mean())
    std_error_test.append(np.array(temp_test).std())
    mean_error_train.append(np.array(temp_train).mean())
    std_error_train.append(np.array(temp_train).std())
    # roc_auc_scores.append(np.array(temp_roc).mean())
#     print('--------')
#     print(np.array(temp_test))
#     print(np.array(temp_test).mean())
# print("----------")
# print(std_error_test)
plt.errorbar(C_range,mean_error_test,yerr=std_error_test,linewidth=3,label="test data") 
plt.errorbar(C_range,mean_error_train,yerr=std_error_test,linewidth=3,label="train data") 
plt.legend()
plt.xlabel('C')
plt.ylabel('Mean square error')
plt.show()

print(x)
print(y)

import random
index = [i for i in range(len(urldata))] 
random.shuffle(index)
urldata = urldata.iloc[index]
urldata
urldata = urldata.drop(urldata[(urldata['url'].str.contains('translate'))].index)
urldata

import numpy as np
from sklearn.model_selection import KFold
from sklearn.svm import LinearSVC
from sklearn.metrics import mean_squared_error
kf = KFold(n_splits=5)
mean_error_train=[]
std_error_train=[]
mean_error_test=[]
std_error_test=[]
train_accuracy = []
test_accuracy = []
C_range = [0.00001,0.0001,0.001,0.01]
for c in C_range:
    temp_train=[]
    temp_test=[]
    accuracy_test = []
    accuracy_train = []
    for train,test in kf.split(x):
        print("=======================")
        print(y.iloc[train])
        model= LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=1, C=c, multi_class='ovr',fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=1, random_state=None, max_iter=1000
    ).fit(x.iloc[train], y.iloc[train])
        ypred_test = model.predict(x.iloc[test])
        ypred_train = model.predict(x.iloc[train])
        accuracy_train.append(model.score(x.iloc[train],y.iloc[train]))
        accuracy_test.append(model.score(x.iloc[test],y.iloc[test]))
        temp_test.append(mean_squared_error(y.iloc[test],ypred_test))
        temp_train.append(mean_squared_error(y.iloc[train],ypred_train))
    mean_error_test.append(np.array(temp_test).mean())
    std_error_test.append(np.array(temp_test).std())
    mean_error_train.append(np.array(temp_train).mean())
    std_error_train.append(np.array(temp_train).std())
    train_accuracy.append(np.mean(accuracy_train))
    test_accuracy.append(np.mean(accuracy_test))
    
plt.errorbar(C_range,mean_error_test,yerr=std_error_test,linewidth=3,label="test data") 
plt.errorbar(C_range,mean_error_train,yerr=std_error_test,linewidth=3,label="train data") 
plt.legend()
plt.xlabel('C')
plt.ylabel('Mean square error')
plt.show()
plt.plot(C_range,train_accuracy)
plt.plot(C_range,test_accuracy)
plt.legend()
plt.xlabel('C')
plt.ylabel('accuracy')
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score

from sklearn.metrics import mean_squared_error
kf = KFold(n_splits=5)
mean_error_train=[]
std_error_train=[]
mean_error_test=[]
std_error_test=[]
train_accuracy = []
test_accuracy = []
ns = [1,5,10,20,30,40,50,60,70,100]
for n in ns: 
    temp_train=[]
    temp_test=[]
    accuracy_test = []
    accuracy_train = []
    for train,test in kf.split(x):
        rfc = RandomForestClassifier(n_estimators = n, verbose = 1)
        rfc.fit(x.iloc[train],y.iloc[train])
        ypred_test = rfc.predict(x.iloc[test])
        ypred_train = rfc.predict(x.iloc[train])
        accuracy_train.append(rfc.score(x.iloc[train],y.iloc[train]))
        accuracy_test.append(rfc.score(x.iloc[test],y.iloc[test]))
        temp_test.append(mean_squared_error(y.iloc[test],ypred_test))
        temp_train.append(mean_squared_error(y.iloc[train],ypred_train))
    mean_error_test.append(np.array(temp_test).mean())
    std_error_test.append(np.array(temp_test).std())
    mean_error_train.append(np.array(temp_train).mean())
    std_error_train.append(np.array(temp_train).std())
    train_accuracy.append(np.mean(accuracy_train))
    test_accuracy.append(np.mean(accuracy_test))
    
plt.errorbar(ns,mean_error_test,yerr=std_error_test,linewidth=3,label="test data") 
plt.errorbar(ns,mean_error_train,yerr=std_error_test,linewidth=3,label="train data") 
plt.legend()
plt.xlabel('C')
plt.ylabel('Mean square error')
plt.show()
plt.plot(ns,train_accuracy)
plt.plot(ns,test_accuracy)
plt.legend()
plt.xlabel('C')
plt.ylabel('accuracy')
plt.show()




# for n in ns:
#     rfc = RandomForestClassifier(n_estimators = n)
#     rfc.fit(X_train, y_train)
#     rfc_predictions = rfc.predict(X_test)
#     accuracy_score(y_test, rfc_predictions)
#     print('==================================================')
#     print("train",rfc.score(X_train,y_train))
#     print("test", rfc.score(X_test,y_test))

#Independent Variables
x = urldata.loc[: ,['url_length','hostname_length','path_length','count_letter']]

#Dependent Variable
y = urldata.iloc[: , 2]

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.callbacks import ModelCheckpoint
import keras
from keras.callbacks import ReduceLROnPlateau
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization ,Activation
from keras.utils import np_utils, to_categorical
from keras.callbacks import ModelCheckpoint
from sklearn.metrics import confusion_matrix,classification_report,roc_auc_score

X_train.shape

model = Sequential()
model.add(Dense(32, activation = 'relu', input_shape = (11, )))

model.add(Dense(16, activation='relu'))

model.add(Dense(8, activation='relu')) 

model.add(Dense(1, activation='sigmoid')) 
model.summary()

opt = keras.optimizers.Adam(lr=0.0001)
model.compile(optimizer= opt ,loss='binary_crossentropy',metrics=['acc'])

#checkpointer = ModelCheckpoint('url.h5', monitor='val_acc', mode='max', verbose=2, save_best_only=True)
history=model.fit(X_train, y_train, batch_size=256, epochs=5, validation_data=(X_test, y_test))

# plot the training artifacts
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train_acc','val_acc'], loc = 'upper right')
plt.show()

# plot the training artifacts
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train_loss','val_loss'], loc = 'upper right')
plt.show()

from sklearn.metrics import confusion_matrix,classification_report,roc_curve,auc
import numpy as np


y_pred_keras = model.predict(X_test).ravel()

fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred_keras)

auc_keras = auc(fpr_keras,tpr_keras)

plt.figure(1)

plt.plot([0, 1], [0, 1], 'k--')

plt.plot(fpr_keras, tpr_keras, label='Neural network (area = {:.3f})'.format(auc_keras))

plt.xlabel('False positive rate')

plt.ylabel('True positive rate')

plt.title('ROC curve')

plt.legend(loc='best')

plt.show()

preds = (model.predict(X_train)>0.5).astype("int32")
print(preds)
print(classification_report(y_train, preds))
print(confusion_matrix(y_train, preds))

preds = (model.predict(X_train)<0.5).astype("int32")
print(preds)
print(classification_report(y_train, preds))
print(confusion_matrix(y_train, preds))

# preds = model.predict(X_test)
# y_pred = np.argmax(preds, axis=1)
# #y_test1 = np.argmax(y_test, axis=1)
# print(classification_report(y_test, y_pred))
# print(confusion_matrix(y_test, y_pred))

import itertools

def plot_confusion_matrix(cm, classes,title='Confusion matrix', cmap=plt.cm.Blues):
    """This function prints and plots the confusion matrix. """
    plt.imshow(cm, interpolation='nearest', cmap=cmap) 
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes)) 
    plt.xticks(tick_marks, classes, rotation=0) 
    plt.yticks(tick_marks, classes)
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j], horizontalalignment="center",
        color="white" if cm[i, j] > thresh else "black")
        plt.tight_layout()
        plt.ylabel('True label')
        plt.xlabel('Predicted label')

# Confusion matrix for train data
ypred_training = (model.predict(X_train)>0.5).astype("int32") 
confusion_matrix_LR = confusion_matrix(y_train,ypred_training) 
class_names = ['benign','malicious']
plt.figure()
plot_confusion_matrix(confusion_matrix_LR, classes=class_names, title='confusion')
plt.show()

# Confusion matrix for test data
ypred_test = (model.predict(X_test)>0.5).astype("int32") 
confusion_matrix_LR = confusion_matrix(y_test,ypred_test) 
class_names = ['benign','malicious']
plt.figure()
plot_confusion_matrix(confusion_matrix_LR, classes=class_names, title='confusion')
plt.show()
